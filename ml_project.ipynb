{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "__Pia CHANCEREL - Raphael LASRY - Maxime POLI__\n",
    "\n",
    "Based on the article :\n",
    "\n",
    "_A Continuation Method for Semi-Supervised SVMs_\n",
    "\n",
    "Olivier Chapelle $\\hspace{3.9cm}$ olivier.chapelle@tuebingen.mpg.de\n",
    "\n",
    "Mingmin Chi $\\hspace{4.5cm}$ mingmin.chi@tuebingen.mpg.de\n",
    "\n",
    "Alexander Zien $\\hspace{4.1cm}$ alexander.zien@tuebingen.mpg.de\n",
    "\n",
    "\n",
    "Max Planck Institute for Biological Cybernetics, Tübingen, Germany\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/1143844.1143868?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20newsgroup\n",
    "\n",
    "20newsgroup dataset: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "Dataset of an old forum. We will only focus on messages related to windows and mac. The goal is to predict the subject of the message (windows or mac) thanks to a $S^3VM$ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# print(newsgroups_train.DESCR) # Documentation of how to use the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['comp.sys.mac.hardware', 'comp.os.ms-windows.misc']\n",
    "# cat = ['sci.crypt', 'sci.electronics']\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train', categories = cat) \n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test', categories = cat) \n",
    "\n",
    "print('training set')\n",
    "print('posts', newsgroups_train.filenames.shape) # Text, content of the messages\n",
    "print('class', newsgroups_train.target.shape) # In which categories the message should be classified\n",
    "print('test set')\n",
    "print('posts', newsgroups_test.filenames.shape)\n",
    "print('class', newsgroups_test.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text to vectors\n",
    "vectorizer = TfidfVectorizer(encoding='latin1')\n",
    "# Pas sûr de ma solution mais ça a l'air d'être ça\n",
    "vectorizer.fit(newsgroups_train.data+newsgroups_test.data) # From a text message to a sparse matrix\n",
    "vectors_train = vectorizer.transform(newsgroups_train.data)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "print(vectors_train.shape)\n",
    "print(vectors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre moyen de features non nulles\n",
    "vectors_test.nnz/float(vectors_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectors_train) # Sparse matrix. The first index is the index of the message and the second ones are the indexes whithin this matrix where the value isn't a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données\n",
    "data = pd.read_csv('data.csv')\n",
    "data['date_int'] = pd.to_datetime(data['date']).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "city = pd.get_dummies(pd.Categorical(data['city']), prefix='city')\n",
    "X_housedata = np.concatenate([data[['date_int', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']].to_numpy(), city.to_numpy()], axis=1)\n",
    "Y_housedata = data['price'].to_numpy()\n",
    "\n",
    "# séparation train, val, test\n",
    "r = np.random.permutation(len(X_housedata))\n",
    "X_train = X_housedata[r[0:3500], :]\n",
    "Y_train = Y_housedata[r[0:3500]]\n",
    "X_val = X_housedata[r[3500:4000], :]\n",
    "Y_val = Y_housedata[r[3500:4000]]\n",
    "X_test = X_housedata[r[4000:], :]\n",
    "Y_test = Y_housedata[r[4000:]]\n",
    "\n",
    "# standardisation\n",
    "X_train = (X_train - np.mean(X_train, axis = 0)) / (np.std(X_train, axis = 0) + 10 ** (-15))\n",
    "X_val = (X_val - np.mean(X_val, axis = 0)) / (np.std(X_val, axis = 0) + 10 ** (-15))\n",
    "X_test = (X_test - np.mean(X_test, axis = 0)) / (np.std(X_test, axis = 0) + 10 ** (-15))\n",
    "\n",
    "# séparation en 2 classes\n",
    "Y_train = 2 * np.array(np.array(Y_train) - np.median(Y_train) > 0).astype(int) - 1\n",
    "Y_val = 2 * np.array(np.array(Y_val) - np.median(Y_val) > 0).astype(int) - 1\n",
    "Y_test = 2 * np.array(np.array(Y_test) - np.median(Y_test) > 0).astype(int) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test de multiplication\n",
    "X = np.ones((3, 4))\n",
    "Y = np.arange(3)\n",
    "print(X, Y)\n",
    "print(np.multiply(X.T, Y).T)\n",
    "print((X.T * Y).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3VMClassifier():\n",
    "    \"\"\"\n",
    "        S3VM class as defined in the article.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=1.0, C_=1.0, lr=0.01, s=3, sparse_data=False):\n",
    "        self.C = C\n",
    "        self.C_ = C_\n",
    "        self.lr = lr\n",
    "        self.s = s\n",
    "        self.loss_list = []\n",
    "        self.sparse_data = sparse_data\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.X_ = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def compute_gamma(self, epochs):\n",
    "        \"\"\"\n",
    "            Compute the list of values of gamma (defined at the §3.2).\n",
    "        \"\"\"\n",
    "        m, d = self.X_.shape\n",
    "\n",
    "        if self.sparse_data:\n",
    "            matrix = sc.sparse.csr_matrix((d, d))\n",
    "            for i in range(m):\n",
    "                matrix += self.X_[i].T@self.X_[i] / sc.sparse.linalg.norm(self.X_[i]) ** 3\n",
    "            lamb_max = np.real(np.sort(sc.sparse.linalg.eigs(matrix, return_eigenvectors=False))[-1])\n",
    "            gamma_0 = (self.C_ * lamb_max) ** (2 / 3) / (2 * self.s) ** (1 / 3)\n",
    "            gamma_end = 1 / (epochs * 2 * self.s * sc.sparse.linalg.norm(self.X_, axis=1).max() ** 2)\n",
    "            \n",
    "        else:\n",
    "            matrix = np.zeros((d, d))\n",
    "            for i in range(m):\n",
    "                matrix += self.X_[i].T@self.X_[i] / np.linalg.norm(self.X_[i]) ** 3\n",
    "            lamb_max = np.sort(np.linalg.eigvals(matrix))[- 1]\n",
    "            gamma_0 = (self.C_ * lamb_max) ** (2 / 3) / (2 * self.s) ** (1 / 3)\n",
    "            gamma_end = 1 / (epochs * 2 * self.s * np.linalg.norm(self.X_, axis=1).max() ** 2)\n",
    "            \n",
    "        gamma_list = [(gamma_end / gamma_0) ** (i / epochs) * gamma_0 for i in range(epochs)]\n",
    "        return gamma_list\n",
    "    \n",
    "    def loss(self, gamma):\n",
    "        \"\"\"\n",
    "        !!! Pas corrigée pour sparse data\n",
    "            Compute the convolved loss (defined at the end of the §3.1).\n",
    "        \"\"\"\n",
    "        d = self.X.shape[1]\n",
    "        if self.sparse_data:\n",
    "            a = 1 + 2 * gamma * self.s * sc.sparse.linalg.norm(self.X_, axis = 1) ** 2\n",
    "            print(self.X.shape, self.w.shape, self.X.dot(self.w).shape)\n",
    "            e = (self.Y * (self.X.dot(self.w) + self.b) - 1) / np.sqrt(2 * gamma) / sc.sparse.linalg.norm(self.X, axis = 1)\n",
    "            L_labelled = self.C * sc.sparse.csr_matrix.sum(gamma * sc.sparse.linalg.norm(self.X, axis = 1) / np.sqrt(2) \n",
    "                                         * (np.exp(- e ** 2) / np.sqrt(np.pi) - e * sc.special.erfc(e)))\n",
    "            L_unlabelled = self.C_ * sc.sparse.csr_matrix.sum(1 / np.sqrt(a) * np.exp(- self.s * (self.X_.dot(self.w) + self.b) ** 2 / a))\n",
    "\n",
    "        else:\n",
    "            a = 1 + 2 * gamma * self.s * np.linalg.norm(self.X_, axis = 1) ** 2\n",
    "            e = (self.Y * (self.X.dot(self.w) + self.b) - 1) / np.sqrt(2 * gamma) / np.linalg.norm(self.X, axis = 1)\n",
    "            L_labelled = self.C * np.sum(gamma * np.linalg.norm(self.X, axis = 1) / np.sqrt(2) \n",
    "                                         * (np.exp(- e ** 2) / np.sqrt(np.pi) - e * sc.special.erfc(e)))\n",
    "            L_unlabelled = self.C_ * np.sum(1 / np.sqrt(a) * np.exp(- self.s * (self.X_.dot(self.w) + self.b) ** 2 / a))\n",
    "\n",
    "        return np.dot(self.w, self.w) / 2 + gamma * d / 2 + L_labelled + L_unlabelled\n",
    "    \n",
    "    def gradient_loss(self, gamma, labelled):\n",
    "        \"\"\"\n",
    "            Compute the gradient of the loss (defined at the end of the §3.1).\n",
    "        \"\"\"\n",
    "        if self.sparse_data:\n",
    "            a = 1 + 2 * gamma * self.s * sc.sparse.linalg.norm(self.X_, axis = 1) ** 2\n",
    "            e = (self.Y * (self.X.dot(self.w.T).todense() + self.b) - 1) / np.sqrt(2 * gamma) / sc.sparse.linalg.norm(self.X, axis = 1)\n",
    "            e = e.T\n",
    "            if labelled:\n",
    "                yy = np.expand_dims(self.Y, axis=1)\n",
    "                ww = np.multiply(sc.special.erfc(e), yy).T\n",
    "                uu = sc.sparse.csr_matrix.multiply(self.X.T, ww)\n",
    "                dL_labelled = self.C / 2 * np.sum(uu.T, axis=0)\n",
    "                return self.w - dL_labelled\n",
    "            else:\n",
    "                xx = (self.X_.dot(self.w.T).todense() + self.b).T\n",
    "                yy =  2 * self.s * xx / a ** (3 / 2)\n",
    "                ww = np.multiply(yy, np.exp(- self.s * np.multiply(xx,xx) / a))\n",
    "                uu = sc.sparse.csr_matrix.multiply(self.X_.T, ww)\n",
    "                dL_unlabelled = self.C_ * np.sum(uu.T, axis=0)\n",
    "                return self.w - dL_unlabelled\n",
    "            \n",
    "        else:\n",
    "            a = 1 + 2 * gamma * self.s * np.linalg.norm(self.X_, axis = 1) ** 2\n",
    "            e = (self.Y * (self.X.dot(self.w) + self.b) - 1) / np.sqrt(2 * gamma) / np.linalg.norm(self.X, axis = 1)\n",
    "            if labelled:\n",
    "                dL_labelled = self.C / 2 * np.sum(np.multiply(self.X.T, sc.special.erfc(e) * self.Y).T, axis=0)\n",
    "                return self.w - dL_labelled\n",
    "            else:\n",
    "                dL_unlabelled = self.C_ * np.sum(np.multiply(self.X_.T, 2 * self.s * (self.X_.dot(self.w) + self.b) / a ** (3 / 2)\n",
    "                                             * np.exp(- self.s * (self.X_.dot(self.w) + self.b) ** 2 / a)), axis=1)\n",
    "                return self.w - dL_unlabelled\n",
    "    \n",
    "    def minimize(self, gamma, epochs):\n",
    "        \"\"\"\n",
    "            Optimise the parameters.\n",
    "        \"\"\"\n",
    "        n, d = self.X.shape\n",
    "        m = self.X_.shape[0]\n",
    "        indexes = np.arange(n + m) - m # positive indexes --> labeled & negative indexes --> unlabeled\n",
    "        if self.sparse_data:\n",
    "            gw = sc.sparse.csr_matrix((n + m, d))\n",
    "            dw = sc.sparse.csr_matrix((1,d))\n",
    "        else:\n",
    "            gw = np.zeros((n + m, d)) # vector of most recent gradient in w\n",
    "            dw = np.zeros(d) # sum of gw\n",
    "        for ep in range(epochs):\n",
    "            np.random.shuffle(indexes)\n",
    "            for k in range(n + m):\n",
    "                i = indexes[k]\n",
    "                labelled = (i >= 0) # labelled or unlabelled\n",
    "                if i < 0 : # if it's the index of an unlabelled data\n",
    "                    i = - i + n - 1 # its index is in gw\n",
    "                dw -= gw[i]\n",
    "                gw[i] = self.gradient_loss(gamma, labelled)\n",
    "                dw += gw[i]\n",
    "                coeff = self.lr / min((k + 1) + ep * (n + m), n + m)\n",
    "                self.w -=  coeff * dw\n",
    "                print(k, end='\\r')\n",
    "    \n",
    "    def fit(self, X, Y, X_, epochs=1, iter_gamma=10): # epochs pour la minimisation, iter_gamma pour le nombre de gamma\n",
    "        \"\"\"\n",
    "            Train the model on X_labelled, Y_labelled, X_unlabelled datas.\n",
    "        \"\"\"       \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "#         # Centering unlabOuielled data\n",
    "#         if self.sparse_data:\n",
    "#             values = X_[X_.nonzero()]\n",
    "#             values -= sc.sparse.spmatrix.mean(values, axis = 1)\n",
    "#             self.X_ = values\n",
    "#         else:\n",
    "#             mean = np.mean(X_, axis = 0)\n",
    "#             X_ -= mean\n",
    "#             X -= mean\n",
    "        \n",
    "        self.X_ = X_\n",
    "        gamma_list = self.compute_gamma(iter_gamma)\n",
    "        \n",
    "        # warm start\n",
    "        if (self.w is None) and (self.b is None) :\n",
    "            _, d = self.X.shape\n",
    "            if self.sparse_data:\n",
    "                self.w = sc.sparse.csr_matrix((1,d))\n",
    "            else:\n",
    "                self.w = np.zeros(d)\n",
    "            self.b = np.mean(Y)\n",
    "        \n",
    "        # continuation method\n",
    "        #for gamma in tqdm(gamma_list):\n",
    "        print(gamma_list)\n",
    "        for gamma in gamma_list:\n",
    "            print('gamma:', gamma)\n",
    "            self.minimize(gamma, epochs)\n",
    "            if not self.sparse_data:\n",
    "                self.loss_list.append(self.loss(gamma))\n",
    "        \n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Predict the value of Y (-1 or 1).\n",
    "        \"\"\"\n",
    "        if self.sparse_data:\n",
    "            return 2*(self.w.dot(X.T).todense()+self.b >= 0)-1\n",
    "        return 2*(self.w.T.dot(X.T) + self.b >= 0)-1\n",
    "    \n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "            Evaluate the accuracy of the prediction.\n",
    "        \"\"\"\n",
    "        return (self.predict(X) == Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour les figures\n",
    "\n",
    "def make_meshgrid(points, h=0.2):\n",
    "    x_min, x_max = points[:,0].min() - 1, points[:,0].max() + 1\n",
    "    y_min, y_max = points[:,1].min() - 1, points[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, model, X, Y, **params):\n",
    "    xx, yy = make_meshgrid(X)\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, **params)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y, s=20, cmap=plt.cm.Spectral)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = 2*(dataset.target) -1\n",
    "\n",
    "r = np.random.permutation(len(X))\n",
    "\n",
    "test_size = 150\n",
    "Xtest = X[r[:test_size]]\n",
    "ytest = y[r[:test_size]]\n",
    "n = 100\n",
    "Xl = X[r[test_size:test_size+n]]\n",
    "yl = y[r[test_size:test_size+n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000\n",
    "Xu = X[r[test_size+n:n+m+test_size]]\n",
    "yu = y[r[test_size+n:n+m+test_size]]\n",
    "# Xl = X[r[test_size+m:]]\n",
    "# yl = y[r[test_size+m:]]\n",
    "print(Xu.shape, Xl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(Xl, yl)\n",
    "print(\"sagc\", np.mean(svm.predict(Xtest) == ytest))\n",
    "\n",
    "s3vm = S3VMClassifier(lr=0.0001)\n",
    "s3vm.fit(Xl, yl, Xu)\n",
    "print(\"s3vm\", s3vm.accuracy(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble jouet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "total_data = 2000\n",
    "proportion = 0.75\n",
    "n = int(proportion * total_data)\n",
    "m = total_data - n\n",
    "sigma = 0.2\n",
    "\n",
    "# labels\n",
    "y = 2 * np.random.randint(2, size=n) - 1\n",
    "y_ = 2 * np.random.randint(2, size=m) - 1 # sert juste à construire x_\n",
    "\n",
    "# observations\n",
    "x = np.zeros((n, 2))\n",
    "for i in range(n):\n",
    "        x[i,:] = np.random.normal((y[i]+1)/2, sigma, 2)\n",
    "x_ = np.zeros((m, 2))\n",
    "for i in range(m):\n",
    "        x_[i,:] = np.random.normal((y_[i]+1)/2, sigma, 2)\n",
    "\n",
    "# classifieur\n",
    "s3vm = S3VMClassifier()\n",
    "s3vm.fit(x, y, x_)\n",
    "print(\"accuracy sur l'ensemble jouet :\", s3vm.accuracy(x, y))\n",
    "\n",
    "# affichage\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plot_contours(ax, s3vm, x, y, alpha=0.8)\n",
    "ax.scatter(x[:, 0], x[:, 1], c=y, s=10, cmap=plt.cm.Spectral)\n",
    "ax.scatter(x_[:, 0], x_[:, 1], c='black', s=10, label='unlabelled')\n",
    "ax.legend()\n",
    "ax.set_title(\"s3vm\")\n",
    "plt.show()\n",
    "\n",
    "# loss\n",
    "plt.plot(np.arange(len(s3vm.loss_list)), np.log10(s3vm.loss_list))\n",
    "plt.xlabel(\"log(loss)\")\n",
    "plt.ylabel(\"epoch\")\n",
    "plt.title(\"evolution de la loss sur le cas jouet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# très long à run (ne run que si nécessaire)\n",
    "\n",
    "def classify_toy(proportion) :\n",
    "\n",
    "    n = int(proportion * total_data)\n",
    "    n = min(n, total_data - 2) # car il faut au moins 2 données non labelisées\n",
    "    n = max(n, 2) # car il faut au moins 2 données labelisées\n",
    "    m = total_data - n\n",
    "\n",
    "    y = 2 * np.random.randint(2, size=n) - 1\n",
    "    x = np.zeros((n, 2))\n",
    "    for i in range(n):\n",
    "            x[i,:] = np.random.normal((y[i]+1)/2, sigma, 2)\n",
    "            \n",
    "    y_ = 2 * np.random.randint(2, size=m) - 1\n",
    "    x_ = np.zeros((m, 2))\n",
    "    for i in range(m):\n",
    "            x_[i,:] = np.random.normal((y_[i]+1)/2, sigma, 2)\n",
    "\n",
    "    s3vm = S3VMClassifier()\n",
    "    s3vm.fit(x, y, x_)\n",
    "    return s3vm.accuracy(x, y)\n",
    "\n",
    "total_data = 2000\n",
    "sigma = 0.6\n",
    "list_proportion = np.linspace(0, 1, 50)\n",
    "list_accuracy = np.zeros(len(list_proportion))\n",
    "for i in range(len(list_proportion)):\n",
    "    print(\"proportion \" + str(i+1) + \"/\" + str(len(list_proportion)))\n",
    "    sum = classify_toy(list_proportion[i])\n",
    "    sum += classify_toy(list_proportion[i])\n",
    "    sum += classify_toy(list_proportion[i])\n",
    "    list_accuracy[i] = sum/3\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "plt.plot(list_proportion, list_accuracy)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"proportion of labelled\")\n",
    "plt.title(\"Evolution de l'accuracy en fonction de la proportion de données labelisées\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6719245550369743, 0.4642729019618588, 0.32079394312985715, 0.22165574065155383, 0.15315522133751938, 0.10582411154339812, 0.07312021416018248, 0.050523133535955486, 0.034909457687037196, 0.024121034280974625]\n",
      "gamma: 0.6719245550369743\n",
      "gamma: 0.4642729019618588\n",
      "412\r"
     ]
    }
   ],
   "source": [
    "s3vm = S3VMClassifier(sparse_data=True)\n",
    "train_labels = 2*newsgroups_train.target[:N] -1\n",
    "test_labels = 2*newsgroups_test.target-1\n",
    "\n",
    "N = vectors_train.shape[0]\n",
    "proportion = 0.95\n",
    "n = int(proportion*N)\n",
    "m = 10\n",
    "\n",
    "s3vm.fit(vectors_train[:n, :], \n",
    "         train_labels[:n], \n",
    "         vectors_train[n:n+m, :], iter_gamma=10)\n",
    "# print(s3vm.accuracy(vectors_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(vectors_train, train_labels)\n",
    "print(np.mean(svm.predict(vectors_test) == test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train[:nb_labelled, :], Y_train[:nb_labelled])\n",
    "print(np.mean(svm.predict(X_test) == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# proportion = 0.8\n",
    "nb_labelled = 1000 #int(proportion * X_train.shape[0])\n",
    "for m in [2, 500, 1000, 2000, 2700]:\n",
    "    s3vm.fit(X_train[:nb_labelled, :], Y_train[:nb_labelled], X_train[nb_labelled:nb_labelled+m, :], iter_gamma = 5, epochs = 3)\n",
    "    print(s3vm.accuracy(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(s3vm.loss_list)), np.log10(s3vm.loss_list))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
